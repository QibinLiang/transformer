# An implementation of TransformerğŸ¤– from the scratch

This project implements the Transformer architecture from the scratch as well as applies the Transformer to the machine translation. The project aims to construct the Transformer in a straightforward way to help me better understand how the Transformer works. I found that learning AI architecture from the paper doesn't really help me to fully understand the paper's idea. There is always something trivial but important behind the idea but never detailed in the paper, which makes me feel there is a huge gap between the paper and the practiceğŸ¤¨. So I create this repo to help me understand the Transformer and its variants, also hope this repo can help someone who is suffering from the Transformer.ğŸ¤—

## Quickstart ğŸš€

train on a single GPU machine
```bash
./run_single.sh
```

parallel train on multi-GPUs 
```bash
./prepare.sh
./run.sh
```

parallel train on multi-GPUs by Slurm
```bash
./prepare.sh
./run_slurm.sh
```

inference
```bash
$ python inference.py 
loading model...
model loaded.
input:    äººå·¥æ™ºèƒ½å¹¶ä¸èƒ½å¸®æˆ‘ä»¬è§£å†³æ‰€æœ‰çš„é—®é¢˜
output:    ai does not help us solve all problems in our bodies
input:    äººå·¥æ™ºèƒ½è§£å†³ä¸€äº›é—®é¢˜    
output:    ai will solve some problems
input:    äººå·¥åªèƒ½è§£å†³ä¸€äº›é—®é¢˜
output:    workers only do something about it
input:    exit
```

## Todo list ğŸ“
- [x] Transformer
- [x] checkpointer
- [x] distributed training
- [ ] beam search
- [ ] prefix beam search
- [ ] evaluate step
- [ ] test step
- [ ] yaml config
- [ ] perplexity

----
## Reference ğŸ“š
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)